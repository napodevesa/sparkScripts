{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8d9860-9f1d-4361-8fb6-e5a7adca38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Verificar si ya existe un SparkContext\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    # Si no existe, crear un nuevo SparkContext\n",
    "    conf = SparkConf().setMaster(\"local\").setAppName(\"transformationAction\")\n",
    "    sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07db4dbb-552a-4dd8-8fdc-f4777434787a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=transformationAction, master=local) created by __init__ at /var/folders/vx/4t63yqrs7rn_1bfgmb3802dr0000gn/T/ipykernel_45316/862466913.py:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m----> 2\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext (master \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m, appName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformationAction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.11/site-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=transformationAction, master=local) created by __init__ at /var/folders/vx/4t63yqrs7rn_1bfgmb3802dr0000gn/T/ipykernel_45316/862466913.py:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext (master = \"local\", appName = \"transformationAction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "978da979-cbbb-4604-ba85-828d38d9d6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3])\n",
    "type (rdd1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82568162-59d0-473a-a279-42903908f3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623222fb-3a75-4fb0-9beb-287456591a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.69:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>transformationAction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=transformationAction>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5de5f2e-ec09-4f51-b1e0-a6fc76afae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.py             \u001b[34mdatasets\u001b[m\u001b[m/        rdd1.ipynb       test2.py\n",
      "Untitled.ipynb   delimiter.ipynb  \u001b[34msparkdata\u001b[m\u001b[m/       text.rtf\n",
      "counter.py       head.py          test1.py         u.data\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43766889-6b9a-420e-9f6f-1740516f4178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'sparkdata'\n",
      "/Users/ndev/Desktop/SparkDev/sparkdata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ndev/anaconda3/envs/bigdata/lib/python3.11/site-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "cd sparkdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93dc0c76-d5ad-4788-804d-d94dd17b8127",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3181799192.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    <\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ls\n",
    "<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df67223c-0610-4cd9-aa04-53472fbe16d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Jupyter vs CLI.ipynb\n",
      "2. Primer RDD.ipynb\n",
      "3. Transformaciones y Acciones sobre RDDs.ipynb\n",
      "4. DF y replicacion.ipynb\n",
      "5. Particionado.ipynb\n",
      "README.md\n",
      "codeExample.py\n",
      "\u001b[34mconfiguracion\u001b[m\u001b[m/\n",
      "data.csv\n",
      "\u001b[34mfiles\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9375ff2-bc3c-40dd-adf1-e2c4946859e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "clear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc6914da-ddc3-41d7-a109-548c5a442fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/ndev/Desktop/sparkScripts/sparkdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fdd6f4f-2ebb-404f-a384-8fc298e7e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/ndev/Desktop/sparkScripts/sparkdata/files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e6dc53d-8e31-4dbd-9632-395a856ed794",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiposOlimpicosrdd = sc.textFile(path + \"paises.csv\").map (lambda line : line.split (\",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3641609-0c54-4fb9-8b12-b215bf499bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id', 'equipo', 'sigla'],\n",
       " ['1', '30. Februar', 'AUT'],\n",
       " ['2', 'A North American Team', 'MEX'],\n",
       " ['3', 'Acipactli', 'MEX'],\n",
       " ['4', 'Acturus', 'ARG']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equiposOlimpicosrdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4937dfb-7a16-4421-a8ad-a970e97a08f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id', 'equipo', 'sigla'],\n",
       " ['1', '30. Februar', 'AUT'],\n",
       " ['2', 'A North American Team', 'MEX'],\n",
       " ['3', 'Acipactli', 'MEX'],\n",
       " ['4', 'Acturus', 'ARG'],\n",
       " ['5', 'Afghanistan', 'AFG'],\n",
       " ['6', 'Akatonbo', 'IRL'],\n",
       " ['7', 'Alain IV', 'SUI'],\n",
       " ['8', 'Albania', 'ALB'],\n",
       " ['9', 'Alcaid', 'POR']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equiposOlimpicosrdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d25b58a-738c-46a7-a0b0-4e1b1c32f3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equiposOlimpicosrdd.map(lambda x: (x[2])).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69392f-e74f-4e4d-bee5-c5a3d40c10cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
